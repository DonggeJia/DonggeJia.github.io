<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Modified from: Deepak Pathak, Jon Barron, and Saurabh Gupta. */
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .center {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 80%;
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  span.highlight {
  background-color: #ffffd0;
  }
  </style>
  <link rel="shortcut icon" href="images/favicon.ico" type="image/vnd.microsoft.icon">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Paul Liang, MIT</title>
  <meta name="Paul Liang's Homepage" http-equiv="Content-Type" content="Paul Liang's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
</head>

<body>
<table width="1000" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Paul Pu Liang</pageheading><br>
    <b>email</b>:&nbsp ppliang (at) mit (dot) edu<br>
    <b>office</b>:&nbsp Wiesner Building E15-392
  </p>

  <tr>
    <td width="33%" valign="top"><img src="images/photo2023_small.jpeg" width="70%" class="center" style="border-radius:15px">
    <p align=center>
    <a href="cv_07_2024.pdf" target="_blank">CV</a> |
    <a href="bio.txt" target="_blank">Bio</a> |
    <a href="https://scholar.google.com/citations?hl=en&user=pKf5LtQAAAAJ">Google Scholar</a> <br/>
    <a href="https://github.com/pliang279">Github</a> |
    <a href="https://twitter.com/pliang279">Twitter</a> <br/>
    <p align=center style="margin-top:-8px;" ><a href="https://twitter.com/pliang279?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @pliang279</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

    <a href="papers/research_statement_paul_liang_2024.pdf">Research</a>, <a href="papers/teaching_statement_paul_liang_2024.pdf">teaching</a>, <a href="papers/diversity_statement_paul_liang_2024.pdf">diversity</a> statements<br/>
    </p>

    </td>
    <td width="68%" valign="top" align="justify">
    <p>I am an Assistant Professor at the <a href="https://www.media.mit.edu/">MIT Media Lab</a> and <a href="https://www.eecs.mit.edu/">MIT EECS</a>, where I direct the Multisensory Intelligence research group.
    </p>
    
    <p>
    In summer 2024, I was a visiting researcher in the <a href="https://simons.berkeley.edu/programs/summer-cluster-ai-psychology-neuroscience">AI, psychology, and neuroscience program</a> at UC Berkeley's Simons Institute for the Theory of Computing.
    Previously, I received my Ph.D. from the <a href="https://www.ml.cmu.edu/">Machine Learning Department</a> at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, advised by <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a> and <a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a>.
    </p>

    <p><b> Prospective students:</b> I am hiring at all levels (post-docs, PhDs, masters, undergrads, and visitors); <a href="https://forms.gle/4ocwQ7PENYHRauBt7">please fill in this form if you are interested</a>.      
    If you want to join MIT as a graduate student, please apply through the programs in <a href="https://www.media.mit.edu/graduate-program/about-media-arts-sciences/">Media Arts & Sciences</a> or <a href="https://www.eecs.mit.edu/academics/graduate-programs/">EECS</a>,
    and mention my name in your application.<br/>
    I'm also happy to collaborate and answer questions about my research and MIT programs, I especially encourage students from underrepresented groups to reach out.
    </p>
    </td>
  </tr>
</table>

<hr/>


<hr/>

<div class='section_div' id='ResearchGroup'>
<table width="100%" width="100%" align="center" border="0" cellspacing="0" cellpadding="8" style="margin-left:15px">
<tr><td colspan="2"><sectionheading>Research Group</sectionheading></td></tr>
<tr><td colspan="2">
  <p>
  Our group studies the foundations of multisensory AI and its impact on the human experience, through three complementary thrusts:
  </p>

  <p>
  (1) Foundations of multisensory AI: The science and engineering of AI systems that can learn and interact with the world through integrating diverse sensory channels.
  </p>

  <p>
  (2) Enhancing human experiences: Designing interactive AI technologies to augment human capabilities and improve overall well-being. 
  </p>

  <p>
  (3) Real-world human-AI interaction: Quantifying and mitigating real-world societal concerns for responsible deployment.
  </p>  
</td></tr>
  <tr>
    <td rowspan="1" width="50%">
      <b>Group</b><br>
      <a href="https://www.antonischristou.com/">Antonis Christou</a> (MAS)<br>
      <a href="https://chanakyaekbote.netlify.app/">Chanakya Ekbote</a> (MAS)<br>
      <a href="https://dd.works/">David Dai</a> (MAS)<br>
      <a href="https://sites.google.com/mit.edu/msjung">Minseok Jung</a> (IDSS MS, co-advised with Lalana Kagal)<br>
      <a href="https://www.linkedin.com/in/devin-murphy-78947318b/">Devin Murphy</a> (EECS MEng, co-advised with Wojciech Matusik)<br>
      <a href="https://scholar.google.com/citations?user=1zL7Q64AAAAJ&hl=en">Lily Chen</a> (EECS MEng)<br>
      <a href="https://www.linkedin.com/in/jimin24/">Jimin Lee</a> (EECS MEng)<br>
      <a href="https://www.linkedin.com/in/peilin-chen-1bb81a22a/">Peilin Chen</a> (EECS MEng)<br>
      <a href="https://web.media.mit.edu/~adithyab/">Adithya Balachandran</a> (EECS MEng)<br>
      <a href="https://www.linkedin.com/in/nidhish-sagar-652769159/">Nidhish Sagar</a> (EECS MEng, co-advised with Richard Braatz)<br>
      <a href="https://stevenshinechen.github.io/">Steven-Shine Chen</a><br>
      <a href="https://www.linkedin.com/in/hengzhi-li-89a17418b/">Hengzhi Li</a><br>
    </td>
  </tr>
  
  <tr><td colspan="2">
      <b>Former students</b><br>
      <a href="https://haofeiyu.me/">Haofei Yu</a>, now PhD student at UIUC<br>
      <a href="https://rpandey.tech/">Rohan Pandey</a>, now at Reworkd AI (YC S23) (best CMU senior thesis award)<br>
      <a href="https://scholar.google.com/citations?user=I4p2ikMAAAAJ&hl=en">Yun Cheng</a>, now PhD student at Princeton<br>
      <a href="https://rulinshao.github.io/">Rulin Shao</a>, now PhD student at University of Washington<br>
      <a href="https://xiangfan.io/">Xiang Fan</a>, now PhD student at the University of Washington (CRA outstanding undergrad researcher honorable mention)<br>
      <a href="https://jivatneet.github.io/">Jivat Neet</a>, then research fellow at Microsoft Research, now PhD student at UC Berkeley<br>
      <a href="https://scholar.google.com/citations?user=fV5fYpsAAAAJ&hl=en">Yiwei Lyu</a>, now PhD student at the University of Michigan (CRA outstanding undergrad researcher honorable mention)<br>
      <a href="https://xiaoyuxin1002.github.io/">Yuxin Xiao</a>, now PhD student at MIT<br>
      <a href="https://peter.onrender.com/">Peter Wu</a>, now PhD student at UC Berkeley<br>
      <a href="https://dongwonl.com/">Dong Won Lee</a>, now PhD student at MIT<br>
      <a href="https://terranceliu.github.io/">Terrance Liu</a>, now PhD student at CMU<br>
      <a href="https://www.linkedin.com/in/cmao/">Chengfeng Mao</a>, now PhD student at MIT<br>
      <a href="https://www.mit.edu/~ziyinl/">Ziyin Liu</a>, then PhD student at the University of Tokyo, now PostDoc at MIT<br>
    </td></tr>
  <tr><td></td></tr>
</table>
</div>

<hr/>

<div class='section_div' id='Teaching'>
<table width="100%" width="100%" align="center" border="0" cellspacing="0" cellpadding="8" style="margin-left:15px">
<tr><td><sectionheading>Teaching</sectionheading></td></tr>
<tr><td>
  Spring 2025: MIT How to AI (Almost) Anything (coming soon!) <br/>
  Spring 2025: MIT Introduction to Machine Learning (coming soon!) <br/>
  Spring 2024: <a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2024/">CMU 11-877 Advanced Topics in Multimodal Machine Learning</a>, with Daniel Fried <br/>
  Fall 2023: <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2023/">CMU 11-777 Multimodal Machine Learning</a>, with Louis-Philippe Morency <br/>
  Summer 2023: African Masters Of Machine Intelligence course on Multimodal AI (<a href="https://docs.google.com/presentation/d/1WIOY5QCjsJoUO8xZ76pKJyydxJb99gAU/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day1</a>, <a href="https://docs.google.com/presentation/d/1vHo4PcdTJwkRiaj4YbdlIvfdUg0p7JBp/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day2</a>, <a href="https://docs.google.com/presentation/d/183Yaxs-1owDSlg906u01k3kt5gZ6wD6X/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day3</a>, <a href="https://docs.google.com/presentation/d/1vhk81MOJ2qLAtcNK2Lp9xeLL8FCCRc-S/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day4</a>)</br>
  2022-2023: <a href="https://cmu-multicomp-lab.github.io/mmml-tutorial/icml2023/">Tutorials on Multimodal ML</a> at ICML, ICMI, CVPR, NAACL with Louis-Philippe Morency <br/>
  Spring 2023: <a href="https://cmu-multicomp-lab.github.io/asi-course/spring2023/">CMU 11-866 Artificial Social Intelligence</a>, with Louis-Philippe Morency <br/>
  Spring 2023: <a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2023/">CMU 11-877 Advanced Topics in Multimodal Machine Learning</a>, with Louis-Philippe Morency <br/>
  Fall 2022: <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2022/">CMU 11-777 Multimodal Machine Learning</a>, with Louis-Philippe Morency <br/>
  Spring 2022: <a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2022/">CMU 11-877 Advanced Topics in Multimodal Machine Learning</a>, with Louis-Philippe Morency, Amir Zadeh <br/></td></tr>
<tr><td></td></tr>
</table>
</div>


<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Selected publications </sectionheading>(see full list <a href="https://pliang279.github.io/papers/">here</a> and on <a href="https://scholar.google.com/citations?hl=en&user=pKf5LtQAAAAJ">Google Scholar</a>)</td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
<br/>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2407.03418">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/hemm.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2407.03418" id="hemm">
    <heading>HEMM: Holistic Evaluation of Multimodal Foundation Models</heading></a><br>
    Paul Pu Liang, Akshay Goindani, Talha Chafekar, Leena Mathur, Haofei Yu, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    NeurIPS 2024<br>
    </p>

    <div class="paper" id="hemm">
    <a href="https://arxiv.org/abs/2407.03418">arXiv</a> |
    <a href="https://github.com/pliang279/HEMM">code</a> |
    <a href="javascript:toggleblock('hemm_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('hemm')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="hemm_abs">Multimodal foundation models that can holistically process text alongside images, video, audio, and other sensory modalities are increasingly used in a variety of real-world applications. However, it is challenging to characterize and study progress in multimodal foundation models, given the range of possible modeling decisions, tasks, and domains. In this paper, we introduce Holistic Evaluation of Multimodal Models (HEMM) to systematically evaluate the capabilities of multimodal foundation models across a set of 3 dimensions: basic skills, information flow, and real-world use cases. Basic multimodal skills are internal abilities required to solve problems, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and the ability to handle external knowledge. Information flow studies how multimodal content changes during a task through querying, translation, editing, and fusion. Use cases span domain-specific challenges introduced in real-world multimedia, affective computing, natural sciences, healthcare, and human-computer interaction applications. Through comprehensive experiments across the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g., basic skills, information flows, and use cases) that pose challenges to today's models, and (2) distill performance trends regarding how different modeling dimensions (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence performance. Our conclusions regarding challenging multimodal interactions, use cases, and tasks requiring reasoning and external knowledge, the benefits of data and model scale, and the impacts of instruction tuning yield actionable insights for future work in multimodal foundation models.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @article{liang2024hemm,
  title={HEMM: Holistic Evaluation of Multimodal Foundation Models},
  author={Liang, Paul Pu and Goindani, Akshay and Chafekar, Talha and Mathur, Leena and Yu, Haofei and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:2407.03418},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2209.03430">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/survey.gif" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2209.03430" id="survey">
    <heading>Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions</heading></a><br>
    Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency<br>
    ACM Computing Surveys 2024, Tutorials at ICML & ICMI 2023, CVPR & NAACL 2022<br>
    </p>

    <div class="paper" id=“survey”>
    <a href="https://arxiv.org/abs/2209.03430">arXiv</a> |
    <a href="https://cmu-multicomp-lab.github.io/mmml-tutorial/schedule/">website</a> |
    <a href="https://www.youtube.com/playlist?list=PLki3HkfgNEsKPcpj5Vv2P98SRAT9wxIDa">videos</a> |
    <a href="javascript:toggleblock('survey_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('survey')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="survey_abs">Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents with intelligent capabilities such as understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in application domains such as healthcare and robotics, multimodal machine learning has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this paper is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. We start by defining three key principles of modality heterogeneity, connections, and interactions that have driven subsequent innovations, and propose a taxonomy of six core technical challenges: representation, alignment, reasoning, generation, transference, and quantification covering historical and recent trends. Recent technical achievements will be presented through the lens of this taxonomy, allowing researchers to understand the similarities and differences across new approaches. We end by motivating several open problems for future research as identified by our taxonomy.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @article{liang2024foundations,
  title={Foundations \& trends in multimodal machine learning: Principles, challenges, and open questions},
  author={Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
  journal={ACM Computing Surveys},
  volume={56},
  number={10},
  pages={1--42},
  year={2024},
  publisher={ACM New York, NY}
}
</pre>
    </div>
  </td>
</tr>  
  
  
<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2302.12247">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/pid.gif" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2302.12247" id="pid">
    <heading>Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework</heading></a><br>
    Paul Pu Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard Chen, Zihao Deng, Nicholas Allen, Randy Auerbach, Faisal Mahmood, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    NeurIPS 2023<br>
    </p>

    <div class="paper" id="pid">
    <a href="https://arxiv.org/abs/2302.12247">arXiv</a> |
    <a href="https://github.com/pliang279/PID">code</a> |
    <a href="javascript:toggleblock('pid_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('pid')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="pid_abs">The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations are compared with human annotations. Finally, we demonstrate their usefulness in (1) quantifying interactions within multimodal datasets, (2) quantifying interactions captured by multimodal models, (3) principled approaches for model selection, and (4) three real-world case studies engaging with domain experts in pathology, mood prediction, and robotic perception where our framework helps to recommend strong multimodal models for each application.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @article{liang2024quantifying,
  title={Quantifying \& modeling multimodal interactions: An information decomposition framework},
  author={Liang, Paul Pu and Cheng, Yun and Fan, Xiang and Ling, Chun Kai and Nie, Suzanne and Chen, Richard and Deng, Zihao and Allen, Nicholas and Auerbach, Randy and Mahmood, Faisal and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>
  
  
<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2203.01311">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/highmmt.gif" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2203.01311" id="highmmt">
    <heading>High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning</heading></a><br>
    Paul Pu Liang, Yiwei Lyu, Xiang Fan, Jeffrey Tsaw, Yudong Liu, Shentong Mo, Dani Yogatama, Louis-Philippe Morency, Ruslan Salakhutdinov<br>
    TMLR 2022<br>
    </p>

    <div class="paper" id="highmmt">
    <a href="https://arxiv.org/abs/2203.01311">arXiv</a> |
    <a href="https://github.com/pliang279/HighMMT">code</a> |
    <a href="javascript:toggleblock('highmmt_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('highmmt')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="highmmt_abs">Many real-world problems are inherently multimodal, from spoken language, gestures, and paralinguistics humans use to communicate, to force, proprioception, and visual sensors on robots. While there has been an explosion of interest in multimodal learning, these methods are focused on a small set of modalities primarily in language, vision, and audio. In order to accelerate generalization towards diverse and understudied modalities, this paper studies efficient representation learning for high-modality scenarios involving a large set of diverse modalities. Since adding new models for every new modality becomes prohibitively expensive, a critical technical challenge is heterogeneity quantification: how can we measure which modalities encode similar information and interactions in order to permit parameter sharing with previous modalities? This paper proposes two new information theoretic metrics for heterogeneity quantification: (1) modality heterogeneity studies how similar 2 modalities {X1,X2} are by measuring how much information can be transferred from X1 to X2, while (2) interaction heterogeneity studies how similarly pairs of modalities {X1,X2}, {X3,X4} interact by measuring how much information can be transferred from fusing {X1,X2} to {X3,X4}. We show the importance of these 2 proposed metrics as a way to automatically prioritize the fusion of modalities that contain unique information or interactions. The result is a single model, HighMMT, that scales up to 10 modalities (text, image, audio, video, sensors, proprioception, speech, time-series, sets, and tables) and 15 tasks from 5 research areas. Not only does HighMMT outperform prior methods on the tradeoff between performance and efficiency, it also demonstrates a crucial scaling behavior: performance continues to improve with each modality added, and it transfers to entirely new modalities and tasks during fine-tuning.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @article{liang2022high,
  title={High-modality multimodal transformer: Quantifying modality \& interaction heterogeneity for high-modality representation learning},
  author={Liang, Paul Pu and Lyu, Yiwei and Fan, Xiang and Tsaw, Jeffrey and Liu, Yudong and Mo, Shentong and Yogatama, Dani and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2203.01311},
  year={2022}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="multibench">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/multibench.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2107.07502" id="multibench">
    <heading>MultiBench: Multiscale Benchmarks for Multimodal Representation Learning</heading></a><br>
    Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Chen, Peter Wu, Michelle Lee, Yuke Zhu, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    NeurIPS 2021, JMLR Open Source Software 2022<br>
    </p>

    <div class="paper" id="multibench">
    <a href="https://arxiv.org/abs/2107.07502">arXiv</a> |
    <a href="https://multibench.readthedocs.io/en/latest/">website</a> |
    <a href="https://github.com/pliang279/MultiBench">code</a> |
    <a href="javascript:toggleblock('multibench_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('multibench')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="multibench_abs">Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. It is a challenging yet crucial area with numerous real-world applications in multimedia, affective computing, robotics, finance, human-computer interaction, and healthcare. Unfortunately, multimodal research has seen limited resources to study (1) generalization across domains and modalities, (2) complexity during training and inference, and (3) robustness to noisy and missing modalities. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiBench, a systematic and unified large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MultiBench provides an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, MultiBench offers a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench introduces impactful challenges for future research, including scalability to large-scale multimodal datasets and robustness to realistic imperfections. To accompany this benchmark, we also provide a standardized implementation of 20 core approaches in multimodal learning. Simply applying methods proposed in different research areas can improve the state-of-the-art performance on 9/15 datasets. Therefore, MultiBench presents a milestone in unifying disjoint efforts in multimodal research and paves the way towards a better understanding of the capabilities and limitations of multimodal models, all the while ensuring ease of use, accessibility, and reproducibility. MultiBench, our standardized code, and leaderboards are publicly available, will be regularly updated, and welcomes inputs from the community.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @article{liang2021multibench,
  title={Multibench: Multiscale benchmarks for multimodal representation learning},
  author={Liang, Paul Pu and Lyu, Yiwei and Fan, Xiang and Wu, Zetian and Cheng, Yun and Wu, Jason and Chen, Leslie and Wu, Peter and Lee, Michelle A and Zhu, Yuke and others},
  journal={Advances in neural information processing systems},
  volume={2021},
  year={2021},
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2106.13219">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/lmbias.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2106.13219" id="lmbias">
    <heading>Towards Understanding and Mitigating Social Biases in Language Models</heading></a><br>
    Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, Ruslan Salakhutdinov<br>
    ICML 2021<br>
    </p>

    <div class="paper" id="lmbias">
    <a href="https://arxiv.org/abs/2106.13219">arXiv</a> |
    <a href="https://github.com/pliang279/LM_bias">code</a> |
    <a href="javascript:toggleblock('lmbias_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('lmbias')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="lmbias_abs">As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{liang2021towards,
  title={Towards understanding and mitigating social biases in language models},
  author={Liang, Paul Pu and Wu, Chiyu and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={International Conference on Machine Learning},
  pages={6565--6576},
  year={2021},
  organization={PMLR}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2001.01523">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/fl.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2001.01523" id="fl">
    <heading>Think Locally, Act Globally: Federated Learning with Local and Global Representations</heading></a><br>
    Paul Pu Liang*, Terrance Liu*, Liu Ziyin, Nicholas Allen, Randy Auerbach, David Brent, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    NeurIPS 2019 Workshop on Federated Learning <font color="#DC143C">(oral, distinguished student paper award) </font> <br>
    </p>

    <div class="paper" id="fl">
    <a href="https://arxiv.org/abs/2001.01523">arXiv</a> |
    <a href="https://github.com/pliang279/LG-FedAvg">code</a> |
    <a href="javascript:toggleblock('fl_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('fl')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="fl_abs">Federated learning is a method of training models on private data distributed over multiple devices. To keep device data private, the global model is trained by only communicating parameters and updates which poses scalability challenges for large models. To this end, we propose a new federated learning algorithm that jointly learns compact local representations on each device and a global model across all devices. As a result, the global model can be smaller since it only operates on local representations, reducing the number of communicated parameters. Theoretically, we provide a generalization analysis which shows that a combination of local and global models reduces both variance in the data as well as variance across device distributions. Empirically, we demonstrate that local models enable communication-efficient training while retaining performance. We also evaluate on the task of personalized mood prediction from real-world mobile data where privacy is key. Finally, local models handle heterogeneous data from new devices, and learn fair representations that obfuscate protected attributes such as race, age, and gender.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @article{liang2020think,
  title={Think locally, act globally: Federated learning with local and global representations},
  author={Liang, Paul Pu and Liu, Terrance and Ziyin, Liu and Allen, Nicholas B and Auerbach, Randy P and Brent, David and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:2001.01523},
  year={2020}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1906.00295">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/mult.gif" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/1906.00295" id="mult">
    <heading>Multimodal Transformer for Unaligned Multimodal Language Sequences</heading></a><br>
    Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, Zico Kolter, Louis-Philippe Morency, Ruslan Salakhutdinov<br>
    ACL 2019<br>
    </p>

    <div class="paper" id="mult">
    <a href="https://arxiv.org/abs/1906.00295">arXiv</a> |
    <a href="https://github.com/yaohungt/Multimodal-Transformer">code</a> |
    <a href="javascript:toggleblock('mult_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('mult')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="mult_abs">Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{tsai2019multimodal,
  title={Multimodal Transformer for Unaligned Multimodal Language Sequences},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={6558--6569},
  year={2019}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://aclanthology.org/P18-1208/">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/006.gif" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://aclanthology.org/P18-1208/" id="mosei">
    <heading>Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph</heading></a><br>
    Amir Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, Louis-Philippe Morency<br>
    ACL 2018 <font color="#DC143C">(oral) </font> <br>
    </p>

    <div class="paper" id="mosei">
    <a href="https://aclanthology.org/P18-1208/">arXiv</a> |
    <a href="https://github.com/CMU-MultiComp-Lab/CMU-MultimodalSDK">code</a> |
    <a href="javascript:toggleblock('mosei_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('mosei')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="mosei_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>


<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right"><font size="1.5">
    Modified version of template from <a href="http://www.cs.berkeley.edu/~barron/">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>

</body>

</html>
